Подключитесь по SSH к мастер-узлу и выполните следующую команду для инициализации нового кластера kubernetes (с заданными параметрами можно ознакомиться изучив файл */etc/kubernetes/kubeadm-config.yaml*, его содержимое также приведено в сценарии Ansible, который Вы использовали для подготовки узлов)
```
sudo kubeadm init --config /etc/kubernetes/kubeadm-config.yaml
```
Просмотрите вывод kubeadm (это действительно может быть полезно, так как он содержит информацию о том, какие дальнейшие действия надо предпринять для получения работоспособного кластера, что впрочем и так будет Вам сообщено далее по мере выполнения данной лабораторной работы) и найдите команду для добавления рабочих узлов (не control-plane), скопируйте ее для дальнейшего использования на других узлах (*Важная информация: в этой команде для авторизации используется временный токен, который действителен 24 часа. При необходимости создать новый токен можно командой ```kubeadm token create --print-join-command```*), она начинается с ```kubeadm join ...```

Скопируйте файл конфигурации кластера для kubectl, чтобы он мог автоматически считывать его (**Запомните, копируемый файл ```admin.conf``` содержит всю необходимую информацию для получения полного доступа к кластеру, его надо беречь**) 
```
cd k8s/
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```
Теперь мы готовы осуществить первые взаимодействия с кластером, давайте сделаем это, для начала убедимся что кластер работает и мы имеем доступ к нему, выполнив 
```
kubectl cluster-info
```
Если все в порядке, в ответ Вы получите информацию о статусе и том, по какому адресу (точнее будет сказать, URL) доступны плоскость управления (control plane) кластера (читай API-сервер) и сервис внутрикластерного DNS.

Давайте проверим теперь состояние узлов нашего кластера, состоящего пока что из единственного мастер-узла, сделать это можно командой 
```
kubectl get nodes
```
Узел находится в статусе *NotReady*, однако больше ничего конкретного относительно данной ситуации мы узнать не сможем, даже если попросим более подробный вывод в том же табличном формате: ```kubectl get nodes -o wide```. Отложим пока ситуацию с узлом на некоторое время в сторону и попробуем выяснить, какие поды работают (и работают ли) сейчас. Выполним команду 
```
kubectl get pods
``` 
и получим ответ, что ресурсов (собственно, самих подов) в пространстве имен по умолчанию (default namespace) не обнаружено. Так как в команде не было указано пространство имен, то и результат мы получили только для пространства имен по умолчанию, которое сейчас пустует, а чтобы получить информацию о ресурсах в других пространствах, их надо указать в запросе kubectl. Для того, чтобы понять, какие у нас пространства имен существуют "из коробки", получим их список
```
kubectl get namespaces
```
В выводе помимо уже знакомого default будет интересующее нас в данный момент kube-system, в котором располагаются объекты, созданные самим кластером, в первую очередь связанные с компонентами плоскости управления.

Попробуем еще раз получить информацию о подах, теперь уже с конкретикой что мы хотим получить данные из пространства имен *kube-system*
```
kubectl get pods -n kube-system
```
Полученная информация должна нас успокоить в том плане, что компоненты плоскости управления кластера запущены и успешно функционируют, однако поды CoreDNS почему-то зависли в состоянии ожидания (Pending), давайте узнаем точную причину происходящего. В этом может помочь команда *kubectl describe*, она используется, чтобы получить подробную информацию о конкретном объекте (объектах), в нашем случае о поде CoreDNS. (*Крайне полезная информация: Чтобы нормально читать вывод этой и некоторых других команд, может потребоваться достаточно сильно увеличить размер окна терминала и возможно понизить размер шрифта*) Полная команда будет выглядеть так (вместо *<pod_name>* подставьте имя любого пода CoreDNS):
```
kubectl describe pods/<pod_name> -n kube-system
``` 
И самый конец вывода, секция События (Events) прямо говорит нам о том, что планировщик не смог найти подходящий узел по причине того, что на единственном доступном узле висит "черная метка", ограничение (taint), к которому у данного пода нет допуска (tolerations). В этом можно убедиться, сравнив пункты из вышележащей секции Tolerations в выведенных данных с вызвавшим проблему ограничением.

Теперь посмотрим, а как же собственно говоря, можно легко и просто получить информацию о всех однотипных ресурсах из всех пространств имен кластера сразу, на примере получения всех существующих на текущий момент в кластере подов:
```
kubectl get pods -A
```
*Полезная информация: узнать какие еще ресурсы кластера существуют помимо ```pod и node```,  можно выполнив команду ```kubectl api-resources``` тут же можно узнать, какие ресурсы namespace-специфичны (как pods), а какие едины для всего кластера (как nodes)*

Итак, настало время разобраться наконец, что не так с нашим узлом, для этого нам снова поможет команда для подробного удобочитаемого вывода состояния объекта 
```
kubectl describe nodes
```
в ее выводе найдем секцию Conditions, а в ней пункт Ready. Данные в колонке Message достаточно прямолинейно говорят о существующей проблеме: не готова к работе сеть подов, так как не проинициализирован плагин CNI. Еще выше можно найти информацию о текущих ограничениях на узле (секция Taints). Зафиксируйте информацию о том, какое еще ограничение висит на узле (преподаватели могут спросить при сдаче работы) и подумайте, для чего оно существует (подсказка: все увиденные нами ранее поды имеют специально прописанный в их спецификациях допуск (толерантность) к этому ограничению, пользовательские нагрузки по умолчанию не имеют допусков).

Теперь, когда мы выявили причину всех теекущих бед с кластером, надо срочно подумать об установке и настройке CNI-плагина, который отвечает за сеть подов, а если быть точнее, за выделение им IP адресов, обеспечение связности между подами на разных узлах (посредством маршрутизации либо туннелирования) и опционально за применение сетевых политик, разграничивающих доступ. В данной работе будет использоваться один из наиболее известных CNI-плагинов, Calico.

Подробно разобрать его принцип работы, особенности и тому подобное мы в рамках данной работы не сможем, поэтому ограничимся краткой информацией: плагин работает в режиме "по умолчанию", используя для передачи данных инкапсуляцию (туннелирование) IPIP (если кратко, на пакет с заголовком, содержащим IP-адрес из подовой подсети, навешивается еще один IP-заголовок, только уже с адресом интерфейса, выходящего во внешнюю сеть) и динамическую маршрутизацию через протокол BGP между всеми узлами. 
Инкапсуляция крайне важна для случая, когда узлы кластера разнесены по разным IP-сетям и трафик между ними должен маршрутизироваться внешними сетевыми устройствами (которые не в курсе про какие-то внутренние сети кластера), в нашем случае "все узлы в одной локальной сети" передача данных между подами работала бы и без инкапсуляции, поскольку коммутатор не смотрит на IP-заголовки. Необходимость в маршрутизации вызывается тем фактом, что единая сеть, выделенная для подсети подов в кластере (она задавалась в конфигурации kubeadm и потом будет указываться в манифесте для установки CNI-плагина Calico) разделяется на небольшие подсети, которые назначаются отдельным узлам в кластере, и, как нетрудно догадаться, если нужно отправить пакет поду из другой подсети, находящемуся на другом узле, узел, с которого пакет отправляется, должен точно знать, на каком именно узле кластера находится данная подсеть подов для адреса назначения.

Для начала установим специальный оператор Kubernetes от разработчиков Calico, который сам развернет все требуемые компоненты CNI-плагина. Оператор в Kubernetes - это приложение, которое следит за установкой и осуществляет управление кастомными ресурсами (определяемыми с помощью манифестов типа CustomResourceDefinition), помогает отслеживать изменения и поддерживать эти ресурсы в желаемом состояние. Tigera operator, который мы установим следующей командой, осуществляет полное управление жизненным циклом Calico в кластере k8s, через него мы можем Calico устанавливать, изменять его конфигурацию, обновлять версию и тд.
```
kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.29.1/manifests/tigera-operator.yaml
```
Если теперь мы снова проверим все поды ```kubectl get pods -A```, то обнаружим, что  в новом простанстве имен появился новый под *tigera-operator...*. Мы можем подождать, пока он полностью запустится, однако больше никаких изменений не произойдет, проблемы с сетью подов останутся на своих местах, а все дело в том, что пока мы не создадим в кластере кастомный ресурс, содержащий параметры установки Calico, оператор ничего делать не будет. Нужная конфигурация была создана заранее на подготовительном этапе и находится в файле *~/k8s/calico.yaml*. Можете открыть и посмотреть содержимое данного файла, наиболее важное там - выделенная для подов сеть (CIDR), которая должна совпадать с указанной в конфигурации kubeadm при инициализации кластера. Применим манифест (*в отличие от прошлой команды kubectl create, kubectl apply позволяет как создать новые ресурсы, так и обновить уже существующие*).
```
kubectl apply -f ./calico.yaml
```
Проверим поды ```kubectl get pods -A```, дабы убедиться в том, что помимо прочих появились *calico-kube-controllers..., calico-node...*. Подождите, пока они проинициализируются и запустятся, после этого также должны запуститьтся поды с CoreDNS, а статус узла из вывода ```kubectl get nodes``` должен наконец стать *Ready*. А раз так, значит мы готовы расширить наш кластер, чтобы он мог называться так обоснованно, поэтому приготовьте ранее сохраненную команду для присоединения нового рабочего узла к кластеру, мы начинаем масштабироваться.

В новой вкладке/новом терминале подключитесь к любому будущему рабочему узлу и выполните ранее сохраненную команду на нем
```
sudo kubeadm join ...
```
На мастер узле, выведя список узлов и список подов, можем заметить, как некоторые поды автоматически развертываются на новом рабочем узле, перед добавлением второго рабочего узла рекомендуем убедиться, что первый уже перешел в статус *Ready*.

Добавьте в кластер второй рабочий узел, действия аналогичны первому

До сих пор мы при необходимости что-то создать/установить в кластере использовали kubectl и файлы с YAML-манифестами, однако это не единственный способ и в некоторых случаях (например, для сложных микросервисных приложений) точно не самый простой. Для Kubernetes существует менеджер пакетов, который подобно пакетному менеджеру Apt для Debian-подобных дистрибутивов Linux, значительно упрощает развертывание, обновление и обслуживание приложений в кластере, используя чарты, хранимые в общедоступных репозиториях. 

Установим сам Helm
```
curl -L https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash
```
Ппробуем установить теперь с его помощью важный инструмент для функционирования горизонтального автомасштабирования нагрузок в кластере, реализующий Metrics API - Metrics server, который помимо упомянутого также позволит получать данные о потребляемых ресурсов подами и загрузке узлов используя команду ```kubectl top <pod | node >```.
```
helm repo add metrics-server https://kubernetes-sigs.github.io/metrics-server
helm repo update
helm show values metrics-server/metrics-server > ./metrics-server.values
```
По умолчанию Metrics server требует, чтобы в кластере функционировал выпуск сертификатов от имени доверенного удостоверяющего центра (Certification Authority), иначе он не будет подключаться к узлам с недоверенными сертификатами, чтобы исправить эту ситуацию, мы можем немного подправить его конфигурацию, для этого мы специально последней выполненной командой сохранили все кастомизируемые при установке Metrics server параметры (сейчас имеющие значения по умолчанию) и теперь мы можем по своему усмотрению править этот аналог файла с переменными для сценария Ansible. В файле *metrics-server.values* найдите приведенный ниже фрагмент текста и добавьте последний, остутствующий по умолчанию параметр
```
defaultArgs:
  - --cert-dir=/tmp
  - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
  - --kubelet-use-node-status-port
  - --metric-resolution=15s
  - --kubelet-insecure-tls
```
Теперь все готово к установке
```
kubectl create ns metrics-server
helm install metrics-server metrics-server/metrics-server -n metrics-server --values ./metrics-server.values 
```
После того, как все созданные поды в пространстве имен *metrics-server* запустятся, можете попробовать получить данные о загрузке узлов
```
kubectl top node
```
*Если возвращается ошибка, немного подождите, сбор метрик при первом запуске Metrics server происходит не быстро*
