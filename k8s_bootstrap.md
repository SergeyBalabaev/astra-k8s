Подключитесь по SSH к мастер-узлу и выполните следующую команду для инициализации нового кластера kubernetes (с заданными параметрами можно ознакомиться изучив файл */etc/kubernetes/kubeadm-config.yaml*, его содержимое также приведено в сценарии Ansible, который Вы использовали для подготовки узлов)
Подключитесь по SSH к мастер-узлу и выполните следующую команду для инициализации нового кластера kubernetes (с заданными параметрами можно ознакомиться изучив файл */etc/kubernetes/kubeadm-config.yaml*, его содержимое также приведено в сценарии Ansible, который Вы использовали для подготовки узлов)
```
sudo kubeadm init --config /etc/kubernetes/kubeadm-config.yaml
```
Просмотрите вывод kubeadm (это действительно может быть полезно, так как он содержит информацию о том, какие дальнейшие действия надо предпринять для получения работоспособного кластера, что впрочем и так будет Вам сообщено далее по мере выполнения данной лабораторной работы) и найдите команду для добавления рабочих узлов (не control-plane), скопируйте ее для дальнейшего использования на других узлах, она начинается с ```kubeadm join ...```(*Важная информация: в этой команде для авторизации используется временный токен, который действителен 24 часа. При необходимости создать новый токен можно командой ```kubeadm token create --print-join-command```*)

Скопируйте файл конфигурации кластера для kubectl, чтобы он мог автоматически считывать его (**Запомните, копируемый файл ```admin.conf``` содержит всю необходимую информацию для получения полного доступа к кластеру, его надо беречь**) 
```
cd k8s/
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```
Теперь мы готовы осуществить первые взаимодействия с кластером, давайте для начала убедимся что кластер работает и мы имеем доступ к нему, выполнив 
```
kubectl cluster-info
```
Если все в порядке, в ответ Вы получите информацию о статусе и том, по какому адресу (точнее будет сказать, URL) доступны плоскость управления (control plane) кластера (читай API-сервер) и сервис внутрикластерного DNS.

Давайте проверим теперь состояние узлов нашего кластера, состоящего пока что из единственного мастер-узла, сделать это можно командой 
```
kubectl get nodes
```
Узел находится в статусе *NotReady*, однако больше ничего конкретного относительно данной ситуации мы узнать пока не можем, даже если попросим более подробный вывод в том же табличном формате: ```kubectl get nodes -o wide```. Отложим пока ситуацию с узлом на некоторое время в сторону и попробуем выяснить, какие поды работают (и работают ли) сейчас в кластере. Выполним команду 
```
kubectl get pods
``` 
и получим ответ, что ресурсов (собственно, самих подов) в пространстве имен по умолчанию (default namespace) не обнаружено. Так как в команде *get* для получения namespace-специфичного ресурса (подов в данном случае) не было указано пространство имен, то и результат мы получили только для пространства имен по умолчанию, которое сейчас пустует.  Чтобы получить информацию о ресурсах в других пространствах имен, их надо указать в запросе kubectl. Дабы понимать, какие у нас пространства имен существуют "из коробки", давайте получим их список
```
kubectl get namespaces
```
В выводе помимо уже знакомого default будет интересующее нас сейчас kube-system, поскольку в нем располагаются объекты, созданные самим кластером, а значит там должны располагаться поды с компонентами кластера.

Попробуем еще раз получить информацию о подах, теперь уже с конкретикой что мы хотим получить данные для пространства имен *kube-system*
```
kubectl get pods -n kube-system
```
Полученная информация должна нас успокоить в том плане, что компоненты кластера запущены и успешно функционируют, однако поды CoreDNS почему-то зависли в состоянии ожидания (Pending), давайте узнаем точную причину происходящего. В этом может помочь команда *kubectl describe*, она используется для того, чтобы получить подробную информацию о конкретном объекте (объектах), в нашем случае о поде CoreDNS. (*Крайне полезная информация: Чтобы нормально читать вывод этой и некоторых других команд, может потребоваться достаточно сильно увеличить размер окна терминала и возможно понизить размер шрифта*) Полная команда будет выглядеть так (вместо *<pod_name>* подставьте имя любого пода CoreDNS):
```
kubectl describe pods/<pod_name> -n kube-system
``` 
И самый конец вывода, секция События (Events) прямо говорит нам о том, что планировщик не смог найти подходящий узел по причине того, что на единственном доступном узле висит "черная метка", ограничение (taint), к которому у данного пода нет допуска (tolerations). В этом можно убедиться, сравнив пункты из вышележащей секции Tolerations в выведенных данных с вызвавшим проблему ограничением.

Теперь посмотрим, а как же собственно говоря, получить информацию о всех однотипных ресурсах из всех пространств имен кластера сразу на примере получения всех существующих на текущий момент в кластере подов:
```
kubectl get pods -A
```
*Полезная информация: узнать какие еще ресурсы кластера существуют помимо ```pod и node```,  можно выполнив команду ```kubectl api-resources``` тут же можно узнать, какие ресурсы namespace-специфичны (как pods), а какие едины для всего кластера (как nodes)*

Итак, настало время разобраться наконец, что не так с нашим узлом, для этого нам снова поможет команда *describe* для подробного удобочитаемого вывода информации об объекте:
```
kubectl describe nodes
```
в ее выводе найдем секцию Conditions, а в ней пункт Ready. Данные в колонке Message достаточно прямолинейно говорят о существующей проблеме: не готова к работе сеть подов, так как не проинициализирован плагин CNI. Еще выше можно найти информацию о текущих ограничениях на узле (секция Taints). Зафиксируйте информацию о том, какое еще ограничение, помимо увиденного в событиях пода CoreDNS ранее, висит на узле (преподаватели могут спросить при сдаче работы) и подумайте, для чего оно существует *(подсказка: все увиденные нами ранее поды имеют специально прописанный в их спецификациях допуск (толерантность) к этому ограничению, пользовательские нагрузки по умолчанию не имеют допусков)*.

Теперь, когда мы выявили причину всех текущих бед с кластером, надо срочно подумать об установке и настройке CNI-плагина, который отвечает за сеть подов, а если быть точнее, за выделение им IP адресов, обеспечение связности между подами на разных узлах (посредством маршрутизации либо инкапсуляции) и опционально за применение сетевых политик, разграничивающих доступ. В данной работе будет использоваться один из наиболее известных CNI-плагинов, Calico.

Подробно разобрать его принцип работы, особенности и тому подобное мы в рамках данной работы не сможем, поэтому ограничимся краткой информацией: плагин работает в режиме "по умолчанию", используя для передачи данных инкапсуляцию (туннелирование) IPIP (если кратко, на пакет с заголовком, содержащим IP-адрес из подовой подсети, навешивается еще один IP-заголовок, только уже с адресом интерфейса, выходящего во внешнюю сеть) и динамическую маршрутизацию через протокол BGP между всеми узлами. 
Инкапсуляция крайне важна для случая, когда узлы кластера разнесены по разным IP-сетям и трафик между ними должен маршрутизироваться внешними сетевыми устройствами (которые не в курсе про какие-то внутренние сети кластера из диапазона "серых" IP адресов), в нашем случае "все узлы в одной локальной сети" передача трафика подов между узлами на самом деле работала бы и без инкапсуляции, поскольку коммутатор не смотрит на IP-заголовки. Необходимость в динамической маршрутизации вызывается тем фактом, что единая IP сеть, выделенная для подсети подов в кластере (она задавалась в конфигурации kubeadm как podSubnet и потом будет указана в манифесте для установки CNI-плагина Calico), разделяется на небольшие подсети, каждая из которых назначается отдельному узлу кластера. И, как нетрудно догадаться, если нужно доставить пакет поду из другой подсети, находящемуся на другом узле, узел, с которого пакет отправляется, должен точно знать, на каком именно узле кластера находится подсеть подов для адреса назначения.

Для начала установим специальный оператор Kubernetes от разработчиков Calico, который сам развернет все требуемые компоненты Calico. Оператор в Kubernetes - это приложение, которое следит за установкой и осуществляет управление кастомными ресурсами (определяемыми с помощью манифестов типа CustomResourceDefinition), помогает отслеживать изменения и поддерживать эти ресурсы в желаемом состояние. Tigera operator, который мы установим следующей командой, осуществляет полное управление жизненным циклом Calico в кластере k8s, через него мы можем Calico устанавливать, изменять его конфигурацию, обновлять версию и тд.
```
kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.29.1/manifests/tigera-operator.yaml
```
Если теперь мы снова проверим все поды ```kubectl get pods -A```, то обнаружим, что  в новом простанстве имен появился новый под *tigera-operator...*. Мы можем подождать, пока он полностью запустится, однако больше никаких изменений не произойдет, проблемы с сетью подов останутся на своих местах, а все дело в том, что пока мы не создадим в кластере кастомный ресурс, содержащий параметры установки Calico, оператор ничего делать не будет. Нужная конфигурация была создана заранее на подготовительном этапе и находится в файле *~/k8s/calico.yaml*. Можете открыть и посмотреть содержимое данного файла, наиболее важное там - выделенная для подов сеть (CIDR), которая должна совпадать с указанной в конфигурации kubeadm при инициализации кластера. Применим манифест (*в отличие от прошлой команды kubectl create, kubectl apply позволяет как создать новые ресурсы, так и обновить уже существующие*).
```
kubectl apply -f ./calico.yaml
```
Проверим поды ```kubectl get pods -A```, дабы убедиться в том, что помимо прочих появились *calico-kube-controllers..., calico-node...*. Подождите, пока они проинициализируются и запустятся, после этого также должны запуститьтся поды с CoreDNS, а статус узла из вывода ```kubectl get nodes``` должен наконец стать *Ready*. А раз так, значит мы готовы расширить наш кластер, чтобы он мог называться так обоснованно, поэтому приготовьте ранее сохраненную команду для присоединения нового рабочего узла к кластеру, мы начинаем масштабироваться.

В новой вкладке/новом терминале подключитесь к любому будущему рабочему узлу и выполните ранее сохраненную команду на нем
```
sudo kubeadm join ...
```
На мастер узле, выведя список узлов и список подов, можем заметить, как некоторые поды автоматически развертываются на новом рабочем узле, перед добавлением второго рабочего узла рекомендуем убедиться, что первый уже перешел в статус *Ready*.

Добавьте в кластер второй рабочий узел, действия аналогичны первому

Давайте попробуем получить данные о текущей загрузке узлов кластера, у kubectl есть встроенная команда для этого:
```
kubectl top node
```
И... мы получаем ошибку, что Metrics API недоступен. "Из коробки" Kubernetes не содержит компонента для реализации этого функционала, но его можно очень легко установить, давайте посмотрим, как именно это сделать.

До сих пор мы при необходимости что-то создать/установить в кластере использовали kubectl и файлы с YAML-манифестами, однако это не единственный способ и в некоторых случаях (например, для сложных микросервисных приложений) точно не самый простой. Для Kubernetes существует менеджер пакетов, который подобно пакетному менеджеру Apt для Debian-подобных дистрибутивов Linux, значительно упрощает развертывание, обновление и обслуживание приложений в кластере, используя чарты, хранимые в общедоступных репозиториях. 

Установим сам Helm
```
curl -L https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash
```
Ппробуем установить теперь с его помощью компонент, реализующий Metrics API - Metrics server, который позволит получать данные о потребляемых ресурсов подами и загрузке узлов используя команду ```kubectl top``` (а также неорбходим для функционирования горизонтального автомасштабирования нагрузок).
```
helm repo add metrics-server https://kubernetes-sigs.github.io/metrics-server
helm repo update
helm show values metrics-server/metrics-server > ./metrics-server.values
```
По умолчанию Metrics server требует, чтобы в кластере функционировал выпуск сертификатов от имени доверенного удостоверяющего центра (Certification Authority), иначе он не будет подключаться к узлам с недоверенными сертификатами, чтобы исправить эту ситуацию, мы можем немного подправить его конфигурацию, для этого мы специально последней выполненной командой сохранили все кастомизируемые при установке Metrics server параметры (сейчас имеющие значения по умолчанию) и теперь мы можем по своему усмотрению править этот аналог файла с переменными для сценария Ansible. В файле *metrics-server.values* найдите приведенный ниже фрагмент текста и добавьте последний, остутствующий по умолчанию параметр
```
defaultArgs:
  - --cert-dir=/tmp
  - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
  - --kubelet-use-node-status-port
  - --metric-resolution=15s
  - --kubelet-insecure-tls
```
Теперь все готово к установке
```
kubectl create ns metrics-server
helm install metrics-server metrics-server/metrics-server -n metrics-server --values ./metrics-server.values 
```
После того, как все созданные поды в пространстве имен *metrics-server* запустятся, можете попробовать получить данные о загрузке узлов
```
kubectl top node
```
*Если возвращается ошибка, немного подождите, сбор метрик при первом запуске Metrics server происходит не быстро*

*Тут далее совсем сырой текст, у меня пока творческий кризис:(*
На пути к полностью готовому к развертыванию пользовательских приложений кластеру нам остался один шаг, заключающийся в обеспечении постоянного хранилища (тома) для данных Stateful-приложений, которые, как известно, должны сохранять свои данные межде перезапусками/обновлениями и тд. Причем крайне желательно, чтобы доступность этого хранилища не зависела от конкретного узла кластера. По аналогии с Docker монтируемые внутрь контейнера в поде называются томами. Тома в контексте Kubernetes бывают как обычными (volumes), которые определяются внутри спецификации пода и их жизненный цикл целиком завязан на таковой у пода, т.е. удалится под, удалится и том, как объект кластера. Для обхода такого неудобства введен специальный ресурс - постоянные тома (persistent volumes). Созданные объекты такого типа имеют свой, независимый от других объектов жизненный цикл, их можно подключать к нескольким подам сразу. Мы можем как использовать как встроенные в k8s драйвера некоторых типов хранилищ, и самостоятельно вручную определять каждый такой том отдельным манифестом, без необходимости доустанавливать что-либо, или использовать автоматическое выделение тома соответствующего класса хранилища (storageclass) по запросу (persistent volume claim), используя внешний Provisioner.

Далее мы продемонстрируем как статическое создание тома, так и динамическое выделение, для которого нам потребуется определить объект storageClass, и в его описании указать Provisioner, к которому кластер будет обращаться за выделением тома с требуемыми параметрами согласно запросу PVC.
Для примера будем использовать NFS сервер а хостовой ВМ в качестве доступного всем узлам хранилища, а Provisioner, который в указанной корневой директории, экспортируемой с сервера NFS, будет создавать по директории на каждый том, установим через Helm.

Для установки NFS запустите соответствующий сценарий на хостовой ВМ
```
ansible-playbook ./nfs_host.yml
```
Provisioner для NFS мы установим еще из одного репозитория helm, он же автоматически создаст класс хранилища по умолчанию
```
helm repo add nfs-subdir-external-provisioner https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner/
kubectl create ns nfs-provisioner
helm -n nfs-provisioner install nfs-provisioner nfs-subdir-external-provisioner/nfs-subdir-external-provisioner \
    --set nfs.server=192.168.122.1 \
    --set nfs.path=/srv/nfs_share \
    --set storageClass.defaultClass=true \
    --set replicaCount=1 \
    --set storageClass.name=nfs \
    --set storageClass.provisionerName=nfs-provisioner 
```
После развертывания экспресс-проверку можно провести так:
```
cat <<EOF | kubectl apply -f -
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: nfs-test-claim
spec:
  storageClassName: nfs
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 10Mi
EOF

cat <<EOF | kubectl apply -f -
kind: Pod
apiVersion: v1
metadata:
  name: nfs-test-pod
spec:
  containers:
  - name: nfs-test
    image: busybox:stable
    command:
      - "/bin/sh"
    args:
      - "-c"
      - "touch /mnt/SUCCESS && exit 0 || exit 1"
    volumeMounts:
      - name: nfs-pvc
        mountPath: "/mnt"
  restartPolicy: "Never"
  volumes:
    - name: nfs-pvc
      persistentVolumeClaim:
        claimName: nfs-test-claim
```
Если все ОК, под ```nfs-test-pod``` будет в состоянии *Completed*, а в директории */srv/nfs_share* появится вложенная директория с файлом SUCCESS внутри.