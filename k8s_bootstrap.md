Подключитесь по SSH к мастер-узлу и выполните следующую команду для инициализации нового кластера kubernetes (с заданными параметрами можно ознакомиться изучив файл */etc/kubernetes/kubeadm-config.yaml*)
```
sudo kubeadm init --config /etc/kubernetes/kubeadm-config.yaml
```
Просмотрите вывод kubeadm (это действительно полезно в данном случае, так как оттуда можно вытащить информацию о том, какие дальнейшие действия надо предпринять для получения работоспособного кластера, что впрочем и так будет Вам сообщено далее в этой работе) и найдите команду для добавления рабочих узлов (не control-plane), скопируйте ее для дальнейшего использования, она начинается с ```kubeadm join ...```

Скопируйте файл конфигурации кластера для kubectl (**Запомните, копируемый файл содержит всю необходимую информацию для получения полного доступа к кластеру, его надо беречь**) 
```
cd k8s/
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```
Мы готовы осуществить первые взаимодействия с кластером, давайте сделаем это, для начала убедимся что имеем доступ к кластеру, выполнив 
```
kubectl cluster-info
```
Если все в порядке, в ответ Вы получите информацию о статусе и том, по какому адресу (точнее будет сказать, URL) доступны плоскость управления (control plane) кластера (читай API-сервер) и сервис внутрикластерного DNS.

Давайте посмотрим, 
Проверим теперь состояние нашего пока единственного узла, сделать это можно командой 
```
kubectl get nodes
```
Узел находится в статусе *NotReady*, однако больше ничего конкретного относительно данной ситуации мы узнать этой командой не сможем, даже если попросим более подробный вывод: ```kubectl get nodes -o wide```. Отложим пока ситуацию со статусом узла на время в сторону и попробуем выяснить, какие поды работают (и работают ли) в нашем кластере. Выполним команду 
```
kubectl get pods
``` 
и получим ответ, что ресурсов (собственно, самих подов) в пространстве имен по умолчанию (default namespace) не обнаружено. Так как не было указано пространство имен, то и результат мы получили только для пространства имен по умолчанию, чтобы получить информацию о ресурсах в других пространствах, их надо указать в запросе. Чтобы понять, какие у нас пространства имен есть, получим их список 
```
kubectl get namespaces
```
, в котором помимо уже знакомого default будет интересующее нас в данный момент kube-system, в котором должны располагаться объекты, созданные самим кластером, в первую очередь связанные с компонентами плоскости управления.

Попробуем еще раз, теперь уже с конкретикой откуда именно мы хотим получить данные 
```
kubectl get pods -n kube-system
```
Вывод должен нас успокоить в том плане, что все компоненты кластера успешно функционируют, однако поды CoreDNS почему-то зависли в состоянии ожидания (Pending), давайте узнаем точную причину этого. В этом может помочь команда kubectl describe, она используется, чтобы получить информацию о конкретном объекте, в нашем случае о поде CoreDNS. (*Крайне полезная информация: Чтобы нормально читать вывод этой и некоторых других команд, может потребоваться достаточно сильно увеличить размер окна терминала и возможно понизить размер шрифта*) Полная команда будет выглядеть так 
```
kubectl describe pods/<pod_name> -n kube-system
``` 
вместо *<pod_name>* подставьте имя любого пода CoreDNS. И самый конец вывода, секция События (Events) говорит нам о том,что планировщик не смог найти подходящий узел по причине того, что на единственном доступном узле висит "черная метка", ограничение, к которому у данного пода нет толерантности. В этом можно убедиться, сравнив пункты из вышележащей секции Tolerations с вызвавшей проблему "меткой".

Итак, настало время разобраться наконец, что не так с нашим узлом, для этого нам снова поможет команда для подробного вывода сотояния объекта 
```
kubectl describe nodes
```
в ее выводе найдем секцию Conditions, а в ней пункт Ready. Колонка Message достаточно прямолинейно говорит о существующей проблеме: не готова к работе сеть подов, так как не проинициализирован плагин CNI. Еще выше можно найти информацию о текущих ограничениях на узле (секция Taints). Зафиксируйте информацию о том, какое еще ограничение висит на узле (преподаватели могут спросить при сдаче работы) и подумайте, для чего оно счуществует (подсказка: все увиденные нами ранее поды имеют специально прописанную в их спецификациях толерантность к этому ограничению, пользовательские нагрузки по умолчанию нетолерантны вообще )

Теперь, когда мы разобрались с причиной всех бед, надо подумать об установке и настройке CNI-плагина, который отвечает за сеть подов, а точнее за выделение им адресов, обеспечение связности подов на разных узлах (посредством маршрутизации либо туннелирования) и опционально применении сетевых политик, разграничивающих доступ. В данной работе будет использоваться один из наиболее известных плагинов, Calico.

Подробно разобрать принцип работы, особенности и тд мы в рамках данной работы не сможем, поэтому ограничимся краткой информацией: в нашем случае 
kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.29.1/manifests/tigera-operator.yaml
kubectl apply -f ./calico.yaml
